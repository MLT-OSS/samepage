/*
 *
 *                                  Apache License
 *                            Version 2.0, January 2004
 *                         https://www.apache.org/licenses/
 *
 *    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
 *
 *    1. Definitions.
 *
 *       "License" shall mean the terms and conditions for use, reproduction,
 *       and distribution as defined by Sections 1 through 9 of this document.
 *
 *       "Licensor" shall mean the copyright owner or entity authorized by
 *       the copyright owner that is granting the License.
 *
 *       "Legal Entity" shall mean the union of the acting entity and all
 *       other entities that control, are controlled by, or are under common
 *       control with that entity. For the purposes of this definition,
 *       "control" means (i) the power, direct or indirect, to cause the
 *       direction or management of such entity, whether by contract or
 *       otherwise, or (ii) ownership of fifty percent (50%) or more of the
 *       outstanding shares, or (iii) beneficial ownership of such entity.
 *
 *       "You" (or "Your") shall mean an individual or Legal Entity
 *       exercising permissions granted by this License.
 *
 *       "Source" form shall mean the preferred form for making modifications,
 *       including but not limited to software source code, documentation
 *       source, and configuration files.
 *
 *       "Object" form shall mean any form resulting from mechanical
 *       transformation or translation of a Source form, including but
 *       not limited to compiled object code, generated documentation,
 *       and conversions to other media types.
 *
 *       "Work" shall mean the work of authorship, whether in Source or
 *       Object form, made available under the License, as indicated by a
 *       copyright notice that is included in or attached to the work
 *       (an example is provided in the Appendix below).
 *
 *       "Derivative Works" shall mean any work, whether in Source or Object
 *       form, that is based on (or derived from) the Work and for which the
 *       editorial revisions, annotations, elaborations, or other modifications
 *       represent, as a whole, an original work of authorship. For the purposes
 *       of this License, Derivative Works shall not include works that remain
 *       separable from, or merely link (or bind by name) to the interfaces of,
 *       the Work and Derivative Works thereof.
 *
 *       "Contribution" shall mean any work of authorship, including
 *       the original version of the Work and any modifications or additions
 *       to that Work or Derivative Works thereof, that is intentionally
 *       submitted to Licensor for inclusion in the Work by the copyright owner
 *       or by an individual or Legal Entity authorized to submit on behalf of
 *       the copyright owner. For the purposes of this definition, "submitted"
 *       means any form of electronic, verbal, or written communication sent
 *       to the Licensor or its representatives, including but not limited to
 *       communication on electronic mailing lists, source code control systems,
 *       and issue tracking systems that are managed by, or on behalf of, the
 *       Licensor for the purpose of discussing and improving the Work, but
 *       excluding communication that is conspicuously marked or otherwise
 *       designated in writing by the copyright owner as "Not a Contribution."
 *
 *       "Contributor" shall mean Licensor and any individual or Legal Entity
 *       on behalf of whom a Contribution has been received by Licensor and
 *       subsequently incorporated within the Work.
 *
 *    2. Grant of Copyright License. Subject to the terms and conditions of
 *       this License, each Contributor hereby grants to You a perpetual,
 *       worldwide, non-exclusive, no-charge, royalty-free, irrevocable
 *       copyright license to reproduce, prepare Derivative Works of,
 *       publicly display, publicly perform, sublicense, and distribute the
 *       Work and such Derivative Works in Source or Object form.
 *
 *    3. Grant of Patent License. Subject to the terms and conditions of
 *       this License, each Contributor hereby grants to You a perpetual,
 *       worldwide, non-exclusive, no-charge, royalty-free, irrevocable
 *       (except as stated in this section) patent license to make, have made,
 *       use, offer to sell, sell, import, and otherwise transfer the Work,
 *       where such license applies only to those patent claims licensable
 *       by such Contributor that are necessarily infringed by their
 *       Contribution(s) alone or by combination of their Contribution(s)
 *       with the Work to which such Contribution(s) was submitted. If You
 *       institute patent litigation against any entity (including a
 *       cross-claim or counterclaim in a lawsuit) alleging that the Work
 *       or a Contribution incorporated within the Work constitutes direct
 *       or contributory patent infringement, then any patent licenses
 *       granted to You under this License for that Work shall terminate
 *       as of the date such litigation is filed.
 *
 *    4. Redistribution. You may reproduce and distribute copies of the
 *       Work or Derivative Works thereof in any medium, with or without
 *       modifications, and in Source or Object form, provided that You
 *       meet the following conditions:
 *
 *       (a) You must give any other recipients of the Work or
 *           Derivative Works a copy of this License; and
 *
 *       (b) You must cause any modified files to carry prominent notices
 *           stating that You changed the files; and
 *
 *       (c) You must retain, in the Source form of any Derivative Works
 *           that You distribute, all copyright, patent, trademark, and
 *           attribution notices from the Source form of the Work,
 *           excluding those notices that do not pertain to any part of
 *           the Derivative Works; and
 *
 *       (d) If the Work includes a "NOTICE" text file as part of its
 *           distribution, then any Derivative Works that You distribute must
 *           include a readable copy of the attribution notices contained
 *           within such NOTICE file, excluding those notices that do not
 *           pertain to any part of the Derivative Works, in at least one
 *           of the following places: within a NOTICE text file distributed
 *           as part of the Derivative Works; within the Source form or
 *           documentation, if provided along with the Derivative Works; or,
 *           within a display generated by the Derivative Works, if and
 *           wherever such third-party notices normally appear. The contents
 *           of the NOTICE file are for informational purposes only and
 *           do not modify the License. You may add Your own attribution
 *           notices within Derivative Works that You distribute, alongside
 *           or as an addendum to the NOTICE text from the Work, provided
 *           that such additional attribution notices cannot be construed
 *           as modifying the License.
 *
 *       You may add Your own copyright statement to Your modifications and
 *       may provide additional or different license terms and conditions
 *       for use, reproduction, or distribution of Your modifications, or
 *       for any such Derivative Works as a whole, provided Your use,
 *       reproduction, and distribution of the Work otherwise complies with
 *       the conditions stated in this License.
 *
 *    5. Submission of Contributions. Unless You explicitly state otherwise,
 *       any Contribution intentionally submitted for inclusion in the Work
 *       by You to the Licensor shall be under the terms and conditions of
 *       this License, without any additional terms or conditions.
 *       Notwithstanding the above, nothing herein shall supersede or modify
 *       the terms of any separate license agreement you may have executed
 *       with Licensor regarding such Contributions.
 *
 *    6. Trademarks. This License does not grant permission to use the trade
 *       names, trademarks, service marks, or product names of the Licensor,
 *       except as required for reasonable and customary use in describing the
 *       origin of the Work and reproducing the content of the NOTICE file.
 *
 *    7. Disclaimer of Warranty. Unless required by applicable law or
 *       agreed to in writing, Licensor provides the Work (and each
 *       Contributor provides its Contributions) on an "AS IS" BASIS,
 *       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 *       implied, including, without limitation, any warranties or conditions
 *       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
 *       PARTICULAR PURPOSE. You are solely responsible for determining the
 *       appropriateness of using or redistributing the Work and assume any
 *       risks associated with Your exercise of permissions under this License.
 *
 *    8. Limitation of Liability. In no event and under no legal theory,
 *       whether in tort (including negligence), contract, or otherwise,
 *       unless required by applicable law (such as deliberate and grossly
 *       negligent acts) or agreed to in writing, shall any Contributor be
 *       liable to You for damages, including any direct, indirect, special,
 *       incidental, or consequential damages of any character arising as a
 *       result of this License or out of the use or inability to use the
 *       Work (including but not limited to damages for loss of goodwill,
 *       work stoppage, computer failure or malfunction, or any and all
 *       other commercial damages or losses), even if such Contributor
 *       has been advised of the possibility of such damages.
 *
 *    9. Accepting Warranty or Additional Liability. While redistributing
 *       the Work or Derivative Works thereof, You may choose to offer,
 *       and charge a fee for, acceptance of support, warranty, indemnity,
 *       or other liability obligations and/or rights consistent with this
 *       License. However, in accepting such obligations, You may act only
 *       on Your own behalf and on Your sole responsibility, not on behalf
 *       of any other Contributor, and only if You agree to indemnify,
 *       defend, and hold each Contributor harmless for any liability
 *       incurred by, or claims asserted against, such Contributor by reason
 *       of your accepting any such warranty or additional liability.
 *
 *    END OF TERMS AND CONDITIONS
 *
 *    APPENDIX: How to apply the Apache License to your work.
 *
 *       To apply the Apache License to your work, attach the following
 *       boilerplate notice, with the fields enclosed by brackets "{}"
 *       replaced with your own identifying information. (Don't include
 *       the brackets!)  The text should be enclosed in the appropriate
 *       comment syntax for the file format. We also recommend that a
 *       file or class name and description of purpose be included on the
 *       same "printed page" as the copyright notice for easier
 *       identification within third-party archives.
 *
 *    Copyright 2024 OnSamepage
 *
 *    Licensed under the Apache License, Version 2.0 (the "License");
 *    you may not use this file except in compliance with the License.
 *    You may obtain a copy of the License at
 *
 *        https://www.apache.org/licenses/LICENSE-2.0
 *
 *    Unless required by applicable law or agreed to in writing, software
 *    distributed under the License is distributed on an "AS IS" BASIS,
 *    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *    See the License for the specific language governing permissions and
 *    limitations under the License.
 */
package ai.samepage.service;

import ai.samepage.common.core.exception.AuditException;
import ai.samepage.common.core.exception.BusinessException;
import ai.samepage.common.core.exception.LimiterException;
import ai.samepage.common.core.helper.JTokkitHelper;
import ai.samepage.common.core.resp.RespCode;
import ai.samepage.common.core.util.JacksonUtils;
import ai.samepage.config.SystemProperties;
import ai.samepage.entity.ChatRoleEnum;
import ai.samepage.entity.ContentTypeEnum;
import ai.samepage.entity.ConvMsgStatistics;
import ai.samepage.entity.ConversationMessage;
import ai.samepage.llm.LlmGatewayService;
import ai.samepage.llm.exception.OpenAiHttpException;
import ai.samepage.llm.model.*;
import ai.samepage.model.BotModelDTO;
import ai.samepage.model.MessageEnum;
import ai.samepage.model.PromptConfigDTO;
import ai.samepage.model.dto.*;
import ai.samepage.model.vo.TemplateConfigDto;
import ai.samepage.model.vo.UploadObjConfigVO;
import ai.samepage.service.impl.ConversationInfoServiceImpl;
import ai.samepage.utils.MessageUtil;
import cn.hutool.core.collection.CollectionUtil;
import cn.hutool.core.util.IdUtil;
import cn.hutool.core.util.StrUtil;
import com.google.common.collect.Lists;
import io.reactivex.rxjava3.core.Flowable;
import lombok.extern.slf4j.Slf4j;
import org.thymeleaf.util.StringUtils;

import java.util.*;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.atomic.AtomicReference;
import java.util.function.Consumer;
import java.util.stream.Collectors;

import static ai.samepage.constant.MessageConstants.INPUT_TEMPLATE_TYPE;
import static ai.samepage.constant.MessageConstants.INPUT_TEXT_TYPE;


/**
 * 基础llm 处理类
 *
 */

@Slf4j
public abstract class AbstractModelHandler
        implements ModelHandler {

    public AbstractModelHandler(LlmGatewayService llmGatewayService,
                                SystemProperties systemProperties,
                                ExecutorService openaiTaskExecutor,
                                RateLimiterService rateLimiterService,
                                ConvMsgStatisticsService statisticsService) {
        this.llmGatewayService = llmGatewayService;
        this.systemProperties = systemProperties;
        this.executorService = openaiTaskExecutor;
        this.rateLimiterService = rateLimiterService;
        this.statisticsService = statisticsService;
    }

    protected final LlmGatewayService llmGatewayService;

    private final SystemProperties systemProperties;

    protected final ExecutorService executorService;


    protected static String SYS_LANG_PLACEHOLDER = "{langPrompt}";

    protected static String PRE_RESULT_PLACEHOLDER = "{preResult}";

    protected static String CONTENT_PLACEHOLDER = "{content}";


    /**
     * 判断轮次
     * ●over_flag: false表示未超过最大长度，true表示超过最大长度，目前最大长度设置为2048；
     * ●token_length: 当前输入文本tokenizer后的长度
     *
     * @param model        模型配置
     * @param chatMessages 历史记录信息
     * @param size         轮数
     * @param length       token 或者 字符串的长度
     * @param message
     * @param transfer
     * @return
     */
    private boolean message(BotModelDTO.ModelInfo model,
                            List<ChatMessage> chatMessages,
                            Integer size,
                            AtomicReference<Integer> length,
                            ChatMessage message, String[] transfer) {
        boolean result = true;
        // 1. 判断轮数，轮数
        Integer maxRound = model.getMaxRound();
        if (Objects.nonNull(maxRound)) {
            if (maxRound < size) {
                // 不需要设置轮数，由传参控制
                result = false;
            }
        } else {
            String text = message.getContent();
            int len = text.length();
            Integer maxRequestLength = model.getMaxRequestLength();
            if (Objects.nonNull(length) && Objects.nonNull(maxRequestLength)) {
                // 判断字符数
                int l = len + length.get();
                if (maxRequestLength < l) {
                    result = false;
                    // 判断是否是第一条消息，如果是则进行截取
                    // 判断是否是第一条数据如果是则进行截取
                    if (chatMessages.isEmpty()) {
                        message.setContent(text.substring(0, len - Math.max(l - maxRequestLength, 10)));
                    }
                } else {
                    // 需要回传 现在的最新字符数
                    length.set(l);
                }
            } else {
                Integer requestToken = model.getMaxRequestToken();
                if (Objects.nonNull(length) && Objects.nonNull(requestToken)) {

                    // 开始 校验token 数
                    Integer tokenLength = token(model.getTokenName(), chatMessages, message);
                    //log.info("model:{},max:{},token:{}", model.getModel(), requestToken, tokenLength);
                    if (tokenLength > 0.75 * requestToken && model.getModel().startsWith("meta-llama")) {
                        transfer[0] = "middle-out";
                    }
                    if (tokenLength > requestToken) {
                        result = false;
                        // 判断是否是第一条数据如果是则进行截取
                        if (chatMessages.isEmpty()) {
                            message.setContent(text.substring(0, len - Math.max((tokenLength - requestToken) / 2, 10)));
                            //log.info("text:{}", message.getContent());
                        }
                    } else {
                        // 回传token 数
                        length.set(tokenLength);
                    }
                }

            }
        }
        return result;


    }

    /**
     * 计算token数
     *
     * @param modelName 模型名
     * @param messages  上文
     * @param message   最后的请求
     * @return
     */
    protected Integer token(String modelName, List<ChatMessage> messages,
                            ChatMessage message) {
        String str;
        if (messages.isEmpty()) {
            str = message.getContent();
        } else {
            StringBuilder sb = new StringBuilder();
            messages.forEach(k -> sb.append(k.getContent()));
            sb.append(message.getContent());
            str = sb.toString();
        }
        if (StrUtil.isBlank(str)) {
            return 0;
        }

        return JTokkitHelper.countTokens(str, modelName);
    }


    protected static final List<ContentTypeEnum> EXCLUDE_CONTENT_TYPE = Arrays.asList(
            ContentTypeEnum.welcome,
            ContentTypeEnum.doc_link,
            ContentTypeEnum.read_doc,
            ContentTypeEnum.virtual_root
    );

    /**
     * 构建消息体信息
     *
     * @param config   配置信息
     * @param transfer 是否transfer 参数
     * @return
     */
    protected List<ChatMessage> createMessage(ModelConfig config, List<ChatMessage> request,
                                              String[] transfer) {
        Boolean isQuestion = null;
        AtomicReference<String> softReference = new AtomicReference();
        Locale locale = config.getLocale();
        List<ConversationMessage> msg = config.getRequest();
        // 轮数
        Integer round = 0;
        // 总的token 数或者字符数
        AtomicReference<Integer> length = new AtomicReference<>(0);
        BotModelDTO.ModelInfo model = config.getModel();
        Integer requestToken = model.getMaxRequestToken();
        Integer maxRequestLength = model.getMaxRequestLength();
        Boolean previewToken = model.getPreviewToken();
        Integer maxRound = model.getMaxRound();
        List<ChatMessage> msgList = Lists.newArrayList();
        if (CollectionUtil.isNotEmpty(request)) {
            msgList.addAll(request);
        }
        end:
        for (int i = msg.size() - 1; i >= 0; i--) {
            ConversationMessage conversationMessage = msg.get(i);
            ContentTypeEnum contentType = conversationMessage.getContentType();
            if (EXCLUDE_CONTENT_TYPE.contains(contentType)) {
                continue;
            }
            MessageEnum status = conversationMessage.getStatus();
            if (Objects.equals(status, MessageEnum.SUCCESS)) {
                // 开始进行token 计算
                if (Objects.isNull(isQuestion)) {
                    isQuestion = true;
                } else if (!Objects.equals(ContentTypeEnum.intent, conversationMessage.getContentType())) {
                    isQuestion = false;
                }
                boolean isCheck = false;
                if (Objects.equals(previewToken, true)
                        && Objects.nonNull(requestToken)
                        && (Objects.equals(ContentTypeEnum.reply_input, contentType)
                        || Objects.equals(ContentTypeEnum.input, contentType))) {
                    String content = conversationMessage.getContent();
                    if (StrUtil.isNotBlank(content)) {
                        List<MsgDTO> msgDTO = JacksonUtils.readValueAsList(content, MsgDTO.class);
                        if (CollectionUtil.isNotEmpty(msgDTO)) {
                            List<TemplateConfigDto> params = config.getParams();
                            List<BasicTemplateConfigDTO> templateConfig = null;
                            if (CollectionUtil.isNotEmpty(params)) {
                                for (TemplateConfigDto param : params) {
                                    templateConfig.add(param);
                                }
                            }
                            CheckTokenDTO check = check(model, config.getPrompt(), templateConfig, msgDTO.toArray(new MsgDTO[0]));
                            if (Objects.nonNull(check)) {
                                isCheck = true;
                                if (requestToken < length.get() + check.getToken()) {
                                    break;
                                } else {
                                    length.set(length.get() + check.getToken());
                                }
                            }
                        }

                    }
                }
                ChatMessage chatMessage = toChatMessage(config, conversationMessage, locale, isQuestion, softReference);
                if (Objects.nonNull(chatMessage)) {
                    // 判断token 数或者 字符数或者 轮数
                    if (Objects.isNull(requestToken)
                            && Objects.isNull(maxRequestLength)
                            && Objects.isNull(maxRound)) {
                        msgList.add(chatMessage);
                        continue;
                    }
                    while (!isCheck) {
                        if (message(model, msgList, round, length, chatMessage, transfer)) {
                            if (Objects.nonNull(maxRound) && Objects.equals(chatMessage.getRole(), ChatMessageRole.USER.value())) {
                                round += 1;
                            }
                            msgList.add(chatMessage);
                            break;
                        } else {
                            if (!isQuestion) {
                                break end;
                            }
                        }
                    }
                }
            }
        }
        if (msgList.size() > 1) {
            if (CollectionUtil.isNotEmpty(request)) {
                msgList.removeAll(request);
            }
            msgList = Lists.reverse(msgList);
        }
        return msgList;
    }

    /**
     * 构建 强求的request 信息
     *
     * @param config 配置及上线文的信息
     * @return
     */
    protected ChatCompletionRequest completionRequest(ModelConfig config) {
        String preResult = config.getPreResult();
        PromptConfigDTO.PromptConfig promptConfig = config.getPrompt();
        String userPrompt = null;
        List<String> systemPrompt = null;
        List<ChatMessage> chatMessages = Lists.newArrayList();
        if (Objects.nonNull(promptConfig)) {
            systemPrompt = promptConfig.getSystemPrompt();
            userPrompt = promptConfig.getUserPrompt();
        }
        String langPrompt = StrUtil.EMPTY;
        String language = config.getLang();
        for (BotLangDTO botLangDTO : config.getBotLang()) {
            if (Objects.equals(botLangDTO.getKey(), language)) {
                langPrompt = botLangDTO.getPrompt();
                log.info("lang:{},prompt:{}", language, langPrompt);
                break;
            }
        }
        if (StrUtil.isNotBlank(langPrompt)) {
            config.setLangPrompt(langPrompt);
        }
        if (CollectionUtil.isNotEmpty(systemPrompt)) {
            for (String prompt : systemPrompt) {
                if (StrUtil.isNotBlank(prompt)) {
                    chatMessages.add(new ChatMessage(ChatMessageRole.SYSTEM.value(),
                            MessageUtil.replaceText(prompt, SYS_LANG_PLACEHOLDER, langPrompt)));
                }
            }
        }
        String[] transfer = new String[1];
        BotModelDTO.ModelInfo model = config.getModel();
        if (StrUtil.isNotBlank(preResult)) {
            // 对上一次 进行排序
            log.info("user prompt :{},{}", userPrompt, PRE_RESULT_PLACEHOLDER);
            String content = preResult;
            if (StringUtils.contains(userPrompt, PRE_RESULT_PLACEHOLDER)) {
                content = MessageUtil.replaceText(userPrompt, PRE_RESULT_PLACEHOLDER, preResult);
                if (StringUtils.contains(content, SYS_LANG_PLACEHOLDER)) {
                    content = MessageUtil.replaceText(content, SYS_LANG_PLACEHOLDER, langPrompt);
                }
            } else {
                throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR, "中间结果没有用到");
            }
            if (StrUtil.isBlank(content)) {
                throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR,
                        StrUtil.format("中间结果的prompt为空，中间结果：{},prompt:{}", preResult, userPrompt));
            }
            chatMessages.add(new ChatMessage(ChatRoleEnum.user.name(), content));
        } else {
            List<ChatMessage> preMsg = model.getPreMsg();
            if (CollectionUtil.isNotEmpty(preMsg)) {
                chatMessages.addAll(preMsg);
            }
            List<ChatMessage> message = createMessage(config, chatMessages, transfer);
            if (message.isEmpty()) {
                throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR, "发送的消息不能为空");
            }
            chatMessages.addAll(message);
            // check 是否为空
        }

        ChatCompletionRequest.ChatCompletionRequestBuilder builder = ChatCompletionRequest
                .builder().model(model.getModel()).messages(chatMessages);

        Double topP = model.getTopP();
        if (Objects.nonNull(topP)) {
            builder.topP(topP);
        }
        Double temperature = model.getTemperature();
        if (Objects.nonNull(temperature)) {
            builder.temperature(temperature);
        }
        Double frequencyPenalty = model.getFrequencyPenalty();
        if (Objects.nonNull(frequencyPenalty)) {
            builder.frequencyPenalty(frequencyPenalty);
        }
        Double presencePenalty = model.getPresencePenalty();
        if (Objects.nonNull(presencePenalty)) {
            builder.presencePenalty(presencePenalty);
        }
        return builder.build();
    }


    /**
     * 渲染模型参数
     *
     * @param params 机器人配置的信息
     * @param values 用户的输入模板信息
     */

    private void rendParams(List<TemplateConfigDto> params, List<TemplateConfigDto> values) {
        if (CollectionUtil.isEmpty(params) || CollectionUtil.isEmpty(values)) {
            return;
        }
        Map<String, TemplateConfigDto> cache = params.stream().collect(Collectors.toMap(TemplateConfigDto::getKey, k -> k));
        for (TemplateConfigDto param : values) {
            String key = param.getKey();
            TemplateConfigDto templateConfigDto = cache.get(key);
            if (Objects.nonNull(templateConfigDto)) {
                templateConfigDto.setValue(param.getValue());
            }
            cache.remove(key);
        }
        if (!cache.isEmpty()) {
            cache.values().forEach(k -> k.setValue(null));
        }
    }

    protected ChatMessage toChatMessage(ModelConfig config,
                                        ConversationMessage conversationMessage,
                                        Locale locale,
                                        Boolean isQuestion, AtomicReference<String> softReference) {
        String content = conversationMessage.getContent();
        if (StrUtil.isBlank(content)) {
            content = conversationMessage.getLongContent();
        }
        List<TemplateConfigDto> params = config.getParams();
        PromptConfigDTO.PromptConfig prompt = config.getPrompt();
        String userPrompt = null;
        if (Objects.nonNull(prompt)) {
            userPrompt = prompt.getUserPrompt();
        }
        // 判断是否是模板调用
        if (CollectionUtil.isNotEmpty(params)) {
            // 整合配置
            switch (conversationMessage.getContentType()) {
                case input:
                case reply_input:
                    List<MsgDTO> msgList = JacksonUtils.readValueAsList(content, MsgDTO.class);
                    rendParams(params, msgList.stream().filter(k -> Objects.equals(INPUT_TEMPLATE_TYPE, k.getType()))
                            .map(MsgDTO::templateValue).collect(Collectors.toList()));
                    break;
                default:
                    throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR, "模板参数配置不能为空");
            }
            // 整合 形成content 字符串
            // 获取用户的content 信息
            if (StrUtil.isBlank(userPrompt)) {
                throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR, "模板参数配置的prompt不能为空");
            }
            content = MessageUtil.toContent(params, userPrompt);
        } else {
            String intentId = conversationMessage.getIntentId();
            String messageId = conversationMessage.getMessageId();
            ContentTypeEnum contentType = conversationMessage.getContentType();
            if (Objects.equals(contentType, ContentTypeEnum.intent)) {
                if (Objects.equals(messageId, intentId)) {
                    //content = "您选择的文本：\n" + content;
                    String text = softReference.get();
                    if (Objects.isNull(text)) {
                        return new ChatMessage(ChatMessageRole.USER.value(), content);
                    } else {
                        return new ChatMessage(ChatMessageRole.USER.value(), StrUtil.format(text, content));
                    }
                } else {
                    if (Objects.equals(conversationMessage.getRole(), ChatRoleEnum.assistant)) {
                        return null;
                    }
                    if (Objects.equals(isQuestion, true)) {
                        String intentContent = systemProperties.getIntentContent(content, locale);
                        if (Objects.equals(intentContent, content)) {
                            content = intentContent;
                            softReference.set(null);
                        } else {
                            softReference.set(intentContent);
                            return null;
                        }
                    }
                }
            } else if (Objects.equals(contentType, ContentTypeEnum.reply_summary)) {
                content = "Doc Summary:\n" + content;
                softReference.set(null);
            } else {
                content = MessageUtil.getContent(conversationMessage);
                softReference.set(null);
            }
            if (isQuestion && StrUtil.isNotBlank(userPrompt)) {
                if (StringUtils.contains(userPrompt, CONTENT_PLACEHOLDER)) {
                    content = MessageUtil.replaceText(userPrompt, CONTENT_PLACEHOLDER, content);
                }
            }
        }
        String langPrompt = config.getLangPrompt();
        if (isQuestion && StringUtils.contains(content, SYS_LANG_PLACEHOLDER)) {
            content = MessageUtil.replaceText(content, SYS_LANG_PLACEHOLDER,
                    Objects.isNull(langPrompt) ? StrUtil.EMPTY : langPrompt);
        }
        if (StrUtil.isBlank(content)) {
            return null;
        }
        return new ChatMessage(conversationMessage.getRole().getChatMessageRole().value(),
                content);
    }

    /**
     * 文案标题前缀
     */
    protected static final String TITLE_PREFIX = "标题：";
    protected static final String TITLE_PREFIX_ENG = "标题:";
    /**
     * 文案标题正文前缀
     */
    protected static final String CONTENT_PREFIX = "文案：";


    private final RateLimiterService rateLimiterService;

    private final ConvMsgStatisticsService statisticsService;

    protected ChatCompletionResult render(ModelConfig config, Flowable<CompletionChunk> flowable) throws AuditException {
        ConversationMessage response = config.getResponse();
        BotModelDTO.ModelInfo model = config.getModel();
        ConversationInfoServiceImpl.Message msg = config.getMsg();
        response.setSource(model.getSourceBotKey());
        response.setModel(model.getModel());
        msg.setSource(model.getSourceBotKey());
        String taskId = null;
        String res = null;
        StringBuilder sb = new StringBuilder();
        ContentTypeEnum contentType = response.getContentType();
        Consumer<ConversationInfoServiceImpl.Message> consumer = config.getConsumer();
        StringBuilder title = null;
        StringBuilder content = null;
        CompletionChunk chunk = null;
        List<MsgDTO> reply = Lists.newArrayList();
        for (CompletionChunk completionChunk : flowable.blockingIterable()) {
            chunk = completionChunk;
            if (Objects.isNull(completionChunk)) {
                break;
            }
            if (Objects.isNull(taskId)) {
                taskId = completionChunk.getId();
            }
            List<CompletionChunk.CompletionChoice> choices = completionChunk.getChoices();
            if (Objects.isNull(choices) || choices.isEmpty()) {
                continue;
            }
            CompletionChunk.CompletionChoice chatCompletionChoice = choices.get(0);
            CompletionChunk.ChatMessage message = chatCompletionChoice.getMessage();
            if (Objects.isNull(message)) {
                continue;
            }
            String role = message.getRole();
            String text = message.getContent();
            if (Objects.isNull(text)) {
                continue;
            }
            sb.append(text);
            if (Objects.equals(contentType, ContentTypeEnum.reply_input)) {
                if ("image".equalsIgnoreCase(role)) {
                    List<UploadObjConfigVO> inputMsg = builderImage(text);
                    if (CollectionUtil.isNotEmpty(inputMsg)) {
                        msg.images(inputMsg.toArray(new UploadObjConfigVO[0]));
                    }
                    log.warn("resp {}, image: {}", inputMsg, text);
                } else if (StrUtil.isNotBlank(text)) {
                    msg.text(text);
                }
                List<MsgDTO> input = msg.getInput();
                if (CollectionUtil.isNotEmpty(input)) {
                    reply.addAll(input);
                }
                consumer.accept(msg);
            } else {
                msg.setContent(text);
                consumer.accept(msg);
            }
        }
        if (Objects.equals(contentType, ContentTypeEnum.reply_input)) {
            MessageUtil.builderContent(response, reply.toArray(new MsgDTO[0]));
            res = response.getContent();
            if (StrUtil.isBlank(res)) {
                res = response.getLongContent();
            }

        } else {
            res = sb.toString();
            response.setContent(res);

        }
        if (res.isEmpty()) {
            log.error("空的结果：{}", Objects.nonNull(chunk) ? JacksonUtils.writeValueAsString(chunk) : chunk);
            throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR, "返回结果为空");
        }
        ChatCompletionResult chatCompletion = new ChatCompletionResult();
        chatCompletion.setId(taskId);
        ChatCompletionChoice chatCompletionChoice = new ChatCompletionChoice();
        chatCompletionChoice.setMessage(new ChatMessage(ChatMessageRole.ASSISTANT.value(), res));
        chatCompletion.setChoices(Collections.singletonList(chatCompletionChoice));
        return chatCompletion;
    }

    /**
     * 执行llm
     *
     * @param isStream    是否是流式
     * @param config      配置文件
     * @param chatRequest 请求信息
     * @return
     * @throws AuditException
     */
    protected ChatCompletionResult llm(boolean isStream,
                                       ModelConfig config,
                                       ChatCompletionRequest chatRequest
    ) throws AuditException {
        if (isStream) {
            // 流式返回
            return render(config, llmGatewayService.streamChatCompletion(chatRequest));
        } else {
            return llmGatewayService.createChatCompletion(chatRequest);
        }

    }


    @Override
    public CheckTokenDTO check(BotModelDTO.ModelInfo modelInfo, PromptConfigDTO.PromptConfig promptConfig, List<BasicTemplateConfigDTO> templateConfig, MsgDTO... inputMsg) {
        if (Objects.isNull(inputMsg) || Objects.isNull(modelInfo)) {
            return new CheckTokenDTO(false);
        }
        String content = toContent(promptConfig, templateConfig, inputMsg);
        if (StrUtil.isBlank(content)) {
            return new CheckTokenDTO(false);
        }
        return checkContent(modelInfo, content);
    }

    /**
     * 渲染 返回的图片
     *
     * @param base64Str 渲染返回的图片base64
     * @return
     */
    protected List<UploadObjConfigVO> builderImage(String base64Str) {
        return Collections.emptyList();
    }

    /**
     * 校验token 计算
     *
     * @param modelInfo 模型信息
     * @param content   文本信息
     * @return
     */
    protected CheckTokenDTO checkContent(BotModelDTO.ModelInfo modelInfo, String content) {
        String model = modelInfo.getModel();
        Integer requestToken = modelInfo.getMaxRequestToken();
        String tokenName = modelInfo.getTokenName();
        if (StrUtil.isBlank(tokenName)) {
            tokenName = model;
        }
        int tokens = JTokkitHelper.countTokens(content, tokenName);
        return new CheckTokenDTO(model, null, tokens > requestToken, tokens);
    }

    /**
     * @param promptConfig   prompt 配置
     * @param templateConfig 模板配置
     * @param inputMsg       输入信息
     * @return 文本列表
     */
    protected String toContent(PromptConfigDTO.PromptConfig promptConfig,
                               List<BasicTemplateConfigDTO> templateConfig,
                               MsgDTO... inputMsg) {
        StringBuilder sb = new StringBuilder();
        String template = null;
        Map<String, BasicTemplateConfigDTO> templateMap = Collections.emptyMap();
        if (Objects.nonNull(templateConfig) && Objects.nonNull(promptConfig)) {
            template = promptConfig.getUserPrompt();
            templateMap = templateConfig
                    .stream()
                    .collect(Collectors.toMap(BasicTemplateConfigDTO::getKey, k -> k));
        }
        for (MsgDTO msgDTO : inputMsg) {
            switch (msgDTO.getType()) {
                case INPUT_TEXT_TYPE:
                    sb.append(msgDTO.textValue());
                    break;
                case INPUT_TEMPLATE_TYPE:
                    TemplateConfigDto templateConfigDto = msgDTO.templateValue();
                    BasicTemplateConfigDTO basicTemplateConfigDTO = templateMap.get(templateConfigDto.getKey());
                    if (Objects.nonNull(basicTemplateConfigDTO)) {
                        templateConfigDto.setDefaultValue(basicTemplateConfigDTO.getDefaultValue());
                        templateConfigDto.setPrompt(basicTemplateConfigDTO.getPrompt());
                    }
                    template = MessageUtil.toContent(Collections.singletonList(templateConfigDto), template);
                    break;
                default:
                    throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR, StrUtil.format("不支持该类型{}", msgDTO.getType()));
            }
        }
        if (StrUtil.isNotBlank(template)) {
            sb.append(template);
        }
        return sb.toString();
    }

    @Override
    public String handler(ModelConfig config,
                          boolean isStream) throws LimiterException, AuditException {
        BotModelDTO.ModelInfo model = config.getModel();
        String rateKey = model.getRateKey();
        if (StrUtil.isNotBlank(rateKey)) {
            log.info("模型{}限流{}", model.getModel(), rateKey);
            if (!rateLimiterService.tryAcquire(rateKey)) {
                throw new LimiterException(StrUtil.format("超过限流器{}的限流", rateKey));
            }
        }
        ChatCompletionRequest chatRequest = config.getChatRequest();
        if (Objects.isNull(chatRequest)) {
            chatRequest = completionRequest(config);
            if (Objects.nonNull(chatRequest)) {
                log.info("请求信息：{}", JacksonUtils.writeValueAsString(chatRequest));
            }
        }
        ChatCompletionResult chatCompletion = null;
        String taskId = null;
        try {
            chatCompletion = llm(isStream, config, chatRequest);
            taskId = chatCompletion.getId();
            return chatCompletion.getChoices().get(0).getMessage().getContent();
        } catch (OpenAiHttpException httpException) {
            log.error(httpException.getMessage(), httpException);
            if (Objects.equals(429, httpException.statusCode)) {
                throw new LimiterException();
            }
            throw new BusinessException(RespCode.INTERNAL_SERVER_ERROR,
                    StrUtil.format("调用openai报错：{}", httpException.getMessage()));
        } finally {
            if (Objects.nonNull(chatCompletion)) {
                log.info("返回结果：{}", JacksonUtils.writeValueAsString(chatCompletion));
                saveToken(config.getResponse(), chatCompletion, taskId);
            }
        }
    }

    /**
     * 保存 token数
     *
     * @param response
     * @param result
     */
    private void saveToken(ConversationMessage response,
                           ChatCompletionResult result, String taskId) {
        Usage usage = result.getUsage();
        ConvMsgStatistics statistics = new ConvMsgStatistics();
        statistics.setModel(response.getModel());
        statistics.setId(IdUtil.getSnowflakeNextId());
        statistics.setConvId(response.getConversationId());
        statistics.setCreateTime(response.getCreateTime());
        statistics.setMsgId(response.getMessageId());
        statistics.setCreateId(response.getCreateId());
        if (Objects.nonNull(usage)) {
            statistics.setRequestToken((int) usage.getPromptTokens());
            statistics.setResponseToken((int) usage.getCompletionTokens());
            executorService.execute(() -> statisticsService.save(statistics));
        } else {
            executorService.execute(() -> {
                getToken(result, taskId, statistics);
                if (Objects.isNull(statistics.getRequestToken())) {
                    statistics.setRequestToken(0);
                }
                if (Objects.isNull(statistics.getResponseToken())) {
                    statistics.setResponseToken(0);
                }
                statisticsService.save(statistics);
            });
        }
    }

    /**
     * 获取token 数
     *
     * @param result     返回结果
     * @param taskId     任务id
     * @param statistics 统计数据
     */
    protected void getToken(ChatCompletionResult result,
                            String taskId,
                            ConvMsgStatistics statistics) {
        return;
    }
}
